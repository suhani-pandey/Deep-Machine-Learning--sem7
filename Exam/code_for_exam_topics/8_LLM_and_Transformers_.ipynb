{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vg_Ar_JfRJSc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### encoder-decoder tranformers and attention mechanism's purpose"
      ],
      "metadata": {
        "id": "SWJRmP058ywy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleEncoder:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.name = \"Encoder\"\n",
        "\n",
        "    def encode(self, input_tokens):\n",
        "        print(f\"Encoder processing: {input_tokens}\")\n",
        "        # In real transformers, this involves self-attention and feed-forward layers\n",
        "        encoded = [f\"encoded_{token}\" for token in input_tokens]\n",
        "        print(f\"Encoder output: {encoded}\")\n",
        "        return encoded\n",
        "\n",
        "class SimpleDecoder:\n",
        "    def __init__(self):\n",
        "        self.name = \"Decoder\"\n",
        "\n",
        "    def decode(self, encoded_input, target_length=3):\n",
        "        print(f\"Decoder using encoded input: {encoded_input}\")\n",
        "        # Decoder generates output tokens one by one\n",
        "        output = []\n",
        "        for i in range(target_length):\n",
        "            next_token = f\"output_{i+1}\"\n",
        "            output.append(next_token)\n",
        "            print(f\"Generated token {i+1}: {next_token}\")\n",
        "        return output"
      ],
      "metadata": {
        "id": "ccp2yg4m83_X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== ENCODER-DECODER DEMO ===\")\n",
        "encoder = SimpleEncoder()\n",
        "decoder = SimpleDecoder()\n",
        "\n",
        "input_sequence = [\"hello\", \"world\"]\n",
        "encoded_sequence = encoder.encode(input_sequence)\n",
        "output_sequence = decoder.decode(encoded_sequence)\n",
        "print(f\"Final output: {output_sequence}\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDYtOErK8-rl",
        "outputId": "3433f679-4659-44f1-911a-fe5ac671c343"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ENCODER-DECODER DEMO ===\n",
            "Encoder processing: ['hello', 'world']\n",
            "Encoder output: ['encoded_hello', 'encoded_world']\n",
            "Decoder using encoded input: ['encoded_hello', 'encoded_world']\n",
            "Generated token 1: output_1\n",
            "Generated token 2: output_2\n",
            "Generated token 3: output_3\n",
            "Final output: ['output_1', 'output_2', 'output_3']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  How Self attention works"
      ],
      "metadata": {
        "id": "DA6cibUv9QWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_self_attention(tokens):\n",
        "    print(\"=== SELF-ATTENTION STEP-BY-STEP ===\")\n",
        "    print(f\"Input tokens: {tokens}\")\n",
        "\n",
        "    # Step 1: Create Query, Key, Value for each token (simplified)\n",
        "    queries = [f\"Q_{token}\" for token in tokens]\n",
        "    keys = [f\"K_{token}\" for token in tokens]\n",
        "    values = [f\"V_{token}\" for token in tokens]\n",
        "\n",
        "    print(f\"Queries: {queries}\")\n",
        "    print(f\"Keys: {keys}\")\n",
        "    print(f\"Values: {values}\")\n",
        "    print()\n",
        "\n",
        "    # Step 2: Calculate attention scores (how much each token should attend to others)\n",
        "    print(\"Attention process for each token:\")\n",
        "    attention_outputs = []\n",
        "\n",
        "    for i, token in enumerate(tokens):\n",
        "        print(f\"\\nToken '{token}' attending to all tokens:\")\n",
        "\n",
        "        # In real implementation, this involves dot products and softmax\n",
        "        # Here we simulate with simple scores\n",
        "        attention_scores = {}\n",
        "        for j, other_token in enumerate(tokens):\n",
        "            # Simulate attention score calculation\n",
        "            score = 1.0 / (abs(i - j) + 1)  # Closer tokens get higher scores\n",
        "            attention_scores[other_token] = score\n",
        "            print(f\"  Attention to '{other_token}': {score:.2f}\")\n",
        "\n",
        "        # Normalize scores (like softmax)\n",
        "        total_score = sum(attention_scores.values())\n",
        "        normalized_scores = {k: v/total_score for k, v in attention_scores.items()}\n",
        "\n",
        "        print(f\"  Normalized attention: {normalized_scores}\")\n",
        "\n",
        "        # Create attended representation (weighted combination of values)\n",
        "        attended_token = f\"attended_{token}\"\n",
        "        attention_outputs.append(attended_token)\n",
        "\n",
        "    print(f\"\\nSelf-attention output: {attention_outputs}\")\n",
        "    return attention_outputs\n",
        "\n",
        "# Demo: Self-attention\n",
        "tokens = [\"I\", \"love\", \"AI\"]\n",
        "attended_tokens = simple_self_attention(tokens)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE7Rxeqc9YSr",
        "outputId": "36658047-617e-4827-daf1-e011f9374c1c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SELF-ATTENTION STEP-BY-STEP ===\n",
            "Input tokens: ['I', 'love', 'AI']\n",
            "Queries: ['Q_I', 'Q_love', 'Q_AI']\n",
            "Keys: ['K_I', 'K_love', 'K_AI']\n",
            "Values: ['V_I', 'V_love', 'V_AI']\n",
            "\n",
            "Attention process for each token:\n",
            "\n",
            "Token 'I' attending to all tokens:\n",
            "  Attention to 'I': 1.00\n",
            "  Attention to 'love': 0.50\n",
            "  Attention to 'AI': 0.33\n",
            "  Normalized attention: {'I': 0.5454545454545455, 'love': 0.27272727272727276, 'AI': 0.18181818181818182}\n",
            "\n",
            "Token 'love' attending to all tokens:\n",
            "  Attention to 'I': 0.50\n",
            "  Attention to 'love': 1.00\n",
            "  Attention to 'AI': 0.50\n",
            "  Normalized attention: {'I': 0.25, 'love': 0.5, 'AI': 0.25}\n",
            "\n",
            "Token 'AI' attending to all tokens:\n",
            "  Attention to 'I': 0.33\n",
            "  Attention to 'love': 0.50\n",
            "  Attention to 'AI': 1.00\n",
            "  Normalized attention: {'I': 0.18181818181818182, 'love': 0.27272727272727276, 'AI': 0.5454545454545455}\n",
            "\n",
            "Self-attention output: ['attended_I', 'attended_love', 'attended_AI']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformer vs RNN"
      ],
      "metadata": {
        "id": "FNw2h4XM9jq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN:\n",
        "    \"\"\"Simplified RNN to show sequential processing\"\"\"\n",
        "    def __init__(self):\n",
        "        self.hidden_state = None\n",
        "\n",
        "    def process_sequence(self, tokens):\n",
        "        print(\"=== RNN PROCESSING (Sequential) ===\")\n",
        "        print(\"Processing one token at a time, left to right:\")\n",
        "\n",
        "        outputs = []\n",
        "        for i, token in enumerate(tokens):\n",
        "            print(f\"Step {i+1}: Processing '{token}'\")\n",
        "            # RNN updates hidden state sequentially\n",
        "            self.hidden_state = f\"hidden_after_{token}\"\n",
        "            output = f\"rnn_output_{i+1}\"\n",
        "            outputs.append(output)\n",
        "            print(f\"  Hidden state: {self.hidden_state}\")\n",
        "            print(f\"  Output: {output}\")\n",
        "\n",
        "        print(f\"Final RNN outputs: {outputs}\")\n",
        "        return outputs\n",
        "\n",
        "class SimpleTransformer:\n",
        "    \"\"\"Simplified Transformer to show parallel processing\"\"\"\n",
        "    def process_sequence(self, tokens):\n",
        "        print(\"=== TRANSFORMER PROCESSING (Parallel) ===\")\n",
        "        print(\"Processing ALL tokens simultaneously:\")\n",
        "\n",
        "        # All tokens processed at once through self-attention\n",
        "        print(\"Step 1: Self-attention on all tokens\")\n",
        "        attended = simple_self_attention(tokens)\n",
        "\n",
        "        print(\"Step 2: Feed-forward processing (parallel)\")\n",
        "        outputs = [f\"transformer_output_{i+1}\" for i in range(len(tokens))]\n",
        "        print(f\"Final Transformer outputs: {outputs}\")\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "ZFvXfPQN9pL6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison Demo\n",
        "print(\"=== ADVANTAGE: TRANSFORMER vs RNN ===\")\n",
        "test_tokens = [\"The\", \"cat\", \"sat\"]\n",
        "\n",
        "rnn = SimpleRNN()\n",
        "transformer = SimpleTransformer()\n",
        "\n",
        "print(\"\\n--- RNN Approach ---\")\n",
        "rnn_outputs = rnn.process_sequence(test_tokens)\n",
        "\n",
        "print(\"\\n--- Transformer Approach ---\")\n",
        "transformer_outputs = transformer.process_sequence(test_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3VRkvBb9s9C",
        "outputId": "0cb85419-9f2d-4870-eeb8-7d373a887967"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ADVANTAGE: TRANSFORMER vs RNN ===\n",
            "\n",
            "--- RNN Approach ---\n",
            "=== RNN PROCESSING (Sequential) ===\n",
            "Processing one token at a time, left to right:\n",
            "Step 1: Processing 'The'\n",
            "  Hidden state: hidden_after_The\n",
            "  Output: rnn_output_1\n",
            "Step 2: Processing 'cat'\n",
            "  Hidden state: hidden_after_cat\n",
            "  Output: rnn_output_2\n",
            "Step 3: Processing 'sat'\n",
            "  Hidden state: hidden_after_sat\n",
            "  Output: rnn_output_3\n",
            "Final RNN outputs: ['rnn_output_1', 'rnn_output_2', 'rnn_output_3']\n",
            "\n",
            "--- Transformer Approach ---\n",
            "=== TRANSFORMER PROCESSING (Parallel) ===\n",
            "Processing ALL tokens simultaneously:\n",
            "Step 1: Self-attention on all tokens\n",
            "=== SELF-ATTENTION STEP-BY-STEP ===\n",
            "Input tokens: ['The', 'cat', 'sat']\n",
            "Queries: ['Q_The', 'Q_cat', 'Q_sat']\n",
            "Keys: ['K_The', 'K_cat', 'K_sat']\n",
            "Values: ['V_The', 'V_cat', 'V_sat']\n",
            "\n",
            "Attention process for each token:\n",
            "\n",
            "Token 'The' attending to all tokens:\n",
            "  Attention to 'The': 1.00\n",
            "  Attention to 'cat': 0.50\n",
            "  Attention to 'sat': 0.33\n",
            "  Normalized attention: {'The': 0.5454545454545455, 'cat': 0.27272727272727276, 'sat': 0.18181818181818182}\n",
            "\n",
            "Token 'cat' attending to all tokens:\n",
            "  Attention to 'The': 0.50\n",
            "  Attention to 'cat': 1.00\n",
            "  Attention to 'sat': 0.50\n",
            "  Normalized attention: {'The': 0.25, 'cat': 0.5, 'sat': 0.25}\n",
            "\n",
            "Token 'sat' attending to all tokens:\n",
            "  Attention to 'The': 0.33\n",
            "  Attention to 'cat': 0.50\n",
            "  Attention to 'sat': 1.00\n",
            "  Normalized attention: {'The': 0.18181818181818182, 'cat': 0.27272727272727276, 'sat': 0.5454545454545455}\n",
            "\n",
            "Self-attention output: ['attended_The', 'attended_cat', 'attended_sat']\n",
            "Step 2: Feed-forward processing (parallel)\n",
            "Final Transformer outputs: ['transformer_output_1', 'transformer_output_2', 'transformer_output_3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_math_demo():\n",
        "    \"\"\"Very simple math behind attention\"\"\"\n",
        "    print(\"\\n=== ATTENTION MATH (SIMPLIFIED) ===\")\n",
        "\n",
        "    # Simple vectors representing words\n",
        "    word_vectors = {\n",
        "        \"cat\": [1, 0, 1],\n",
        "        \"sat\": [0, 1, 1],\n",
        "        \"mat\": [1, 1, 0]\n",
        "    }\n",
        "\n",
        "    print(\"Word vectors:\")\n",
        "    for word, vec in word_vectors.items():\n",
        "        print(f\"  {word}: {vec}\")\n",
        "\n",
        "    print(\"\\nCalculating attention between 'cat' and other words:\")\n",
        "\n",
        "    cat_vec = word_vectors[\"cat\"]\n",
        "    for word, vec in word_vectors.items():\n",
        "        # Dot product as similarity measure\n",
        "        similarity = sum(a * b for a, b in zip(cat_vec, vec))\n",
        "        print(f\"  'cat' ⋅ '{word}' = {similarity}\")\n",
        "\n",
        "    print(\"\\nHigher dot product = higher attention weight\")\n",
        "    print(\"This helps model understand which words are most relevant!\")\n",
        "\n",
        "attention_math_demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTPP23M19ytE",
        "outputId": "ab2d051b-49c1-4d14-f107-a223268ca2a1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ATTENTION MATH (SIMPLIFIED) ===\n",
            "Word vectors:\n",
            "  cat: [1, 0, 1]\n",
            "  sat: [0, 1, 1]\n",
            "  mat: [1, 1, 0]\n",
            "\n",
            "Calculating attention between 'cat' and other words:\n",
            "  'cat' ⋅ 'cat' = 2\n",
            "  'cat' ⋅ 'sat' = 1\n",
            "  'cat' ⋅ 'mat' = 1\n",
            "\n",
            "Higher dot product = higher attention weight\n",
            "This helps model understand which words are most relevant!\n"
          ]
        }
      ]
    }
  ]
}