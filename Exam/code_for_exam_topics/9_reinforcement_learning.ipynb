{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Z9boudmj-b19"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reinforcement learning components"
      ],
      "metadata": {
        "id": "0mWPAHyj_NoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleEnvironment:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.grid_size = 4\n",
        "        self.agent_pos = [0, 0]\n",
        "        self.goal_pos = [3, 3]   # Goal position\n",
        "        self.obstacles = [[1, 1], [2, 1]]  # Obstacle positions\n",
        "\n",
        "        print(\"=== ENVIRONMENT SETUP ===\")\n",
        "        print(f\"Grid size: {self.grid_size}x{self.grid_size}\")\n",
        "        print(f\"Agent starts at: {self.agent_pos}\")\n",
        "        print(f\"Goal is at: {self.goal_pos}\")\n",
        "        print(f\"Obstacles at: {self.obstacles}\")\n",
        "        self.print_grid()\n",
        "        print()\n",
        "\n",
        "    def get_state(self):\n",
        "        return tuple(self.agent_pos)\n",
        "\n",
        "    def get_possible_actions(self):\n",
        "        return ['up', 'down', 'left', 'right']\n",
        "\n",
        "    def step(self, action):\n",
        "        print(f\"Environment processing action: {action}\")\n",
        "\n",
        "        # Calculate new position based on action\n",
        "        new_pos = self.agent_pos.copy()\n",
        "        if action == 'up' and new_pos[0] > 0:\n",
        "            new_pos[0] -= 1\n",
        "        elif action == 'down' and new_pos[0] < self.grid_size - 1:\n",
        "            new_pos[0] += 1\n",
        "        elif action == 'left' and new_pos[1] > 0:\n",
        "            new_pos[1] -= 1\n",
        "        elif action == 'right' and new_pos[1] < self.grid_size - 1:\n",
        "            new_pos[1] += 1\n",
        "\n",
        "        # Check if new position is valid (not an obstacle)\n",
        "        if new_pos not in self.obstacles:\n",
        "            self.agent_pos = new_pos\n",
        "\n",
        "        # Calculate reward\n",
        "        reward = self.calculate_reward()\n",
        "\n",
        "        # Check if episode is done\n",
        "        done = (self.agent_pos == self.goal_pos)\n",
        "\n",
        "        new_state = self.get_state()\n",
        "        print(f\"New state: {new_state}, Reward: {reward}, Done: {done}\")\n",
        "\n",
        "        return new_state, reward, done\n",
        "\n",
        "    def calculate_reward(self):\n",
        "        if self.agent_pos == self.goal_pos:\n",
        "            return 100  # Big positive reward for reaching goal\n",
        "        elif self.agent_pos in self.obstacles:\n",
        "            return -10  # Negative reward for hitting obstacle\n",
        "        else:\n",
        "            return -1   # Small negative reward for each step (encourages efficiency)\n",
        "\n",
        "    def print_grid(self):\n",
        "        for i in range(self.grid_size):\n",
        "            row = \"\"\n",
        "            for j in range(self.grid_size):\n",
        "                if [i, j] == self.agent_pos:\n",
        "                    row += \"A \"  # Agent\n",
        "                elif [i, j] == self.goal_pos:\n",
        "                    row += \"G \"  # Goal\n",
        "                elif [i, j] in self.obstacles:\n",
        "                    row += \"X \"  # Obstacle\n",
        "                else:\n",
        "                    row += \". \"  # Empty space\n",
        "            print(row)\n",
        "        print()"
      ],
      "metadata": {
        "id": "H8zLtFrh_SDy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleAgent:\n",
        "    def __init__(self, name=\"Simple Agent\"):\n",
        "        self.name = name\n",
        "        self.total_reward = 0\n",
        "        print(f\"=== AGENT: {self.name} ===\")\n",
        "        print(\"Agent initialized and ready to learn!\")\n",
        "\n",
        "    def choose_action(self, state, possible_actions):\n",
        "        action = random.choice(possible_actions)\n",
        "        print(f\"Agent choosing action: {action}\")\n",
        "        return action\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        self.total_reward += reward\n",
        "        print(f\"Agent learned: State {state} → Action {action} → Reward {reward}\")\n",
        "        print(f\"Total reward so far: {self.total_reward}\")"
      ],
      "metadata": {
        "id": "fxxtnR-6_ht5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create environment and agent\n",
        "env = SimpleEnvironment()\n",
        "agent = SimpleAgent(\"Explorer\")\n",
        "\n",
        "print(\"=== STATES ===\")\n",
        "print(\"States represent the current situation/configuration\")\n",
        "current_state = env.get_state()\n",
        "print(f\"Current state: {current_state}\")\n",
        "print(\"In our grid world, state = agent's position [row, col]\")\n",
        "print()\n",
        "\n",
        "print(\"=== ACTIONS ===\")\n",
        "print(\"Actions are choices the agent can make\")\n",
        "possible_actions = env.get_possible_actions()\n",
        "print(f\"Possible actions: {possible_actions}\")\n",
        "print(\"Each action moves the agent in a direction\")\n",
        "print()\n",
        "\n",
        "# Simulate a few steps\n",
        "print(\"=== AGENT-ENVIRONMENT INTERACTION ===\")\n",
        "for step in range(3):\n",
        "    print(f\"\\n--- Step {step + 1} ---\")\n",
        "    state = env.get_state()\n",
        "    action = agent.choose_action(state, possible_actions)\n",
        "    next_state, reward, done = env.step(action)\n",
        "    agent.learn(state, action, reward, next_state)\n",
        "    env.print_grid()\n",
        "\n",
        "    if done:\n",
        "        print(\"Episode finished!\")\n",
        "        break\n",
        "\n",
        "print(\"\\n=== RETURNS ===\")\n",
        "print(\"Returns = Total accumulated reward over time\")\n",
        "print(f\"Agent's total return: {agent.total_reward}\")\n",
        "print(\"Goal: Learn to maximize expected returns\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSaqWQOu_oVV",
        "outputId": "117ee093-c05a-40ae-ed83-f97c00bd285c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ENVIRONMENT SETUP ===\n",
            "Grid size: 4x4\n",
            "Agent starts at: [0, 0]\n",
            "Goal is at: [3, 3]\n",
            "Obstacles at: [[1, 1], [2, 1]]\n",
            "A . . . \n",
            ". X . . \n",
            ". X . . \n",
            ". . . G \n",
            "\n",
            "\n",
            "=== AGENT: Explorer ===\n",
            "Agent initialized and ready to learn!\n",
            "=== STATES ===\n",
            "States represent the current situation/configuration\n",
            "Current state: (0, 0)\n",
            "In our grid world, state = agent's position [row, col]\n",
            "\n",
            "=== ACTIONS ===\n",
            "Actions are choices the agent can make\n",
            "Possible actions: ['up', 'down', 'left', 'right']\n",
            "Each action moves the agent in a direction\n",
            "\n",
            "=== AGENT-ENVIRONMENT INTERACTION ===\n",
            "\n",
            "--- Step 1 ---\n",
            "Agent choosing action: up\n",
            "Environment processing action: up\n",
            "New state: (0, 0), Reward: -1, Done: False\n",
            "Agent learned: State (0, 0) → Action up → Reward -1\n",
            "Total reward so far: -1\n",
            "A . . . \n",
            ". X . . \n",
            ". X . . \n",
            ". . . G \n",
            "\n",
            "\n",
            "--- Step 2 ---\n",
            "Agent choosing action: right\n",
            "Environment processing action: right\n",
            "New state: (0, 1), Reward: -1, Done: False\n",
            "Agent learned: State (0, 0) → Action right → Reward -1\n",
            "Total reward so far: -2\n",
            ". A . . \n",
            ". X . . \n",
            ". X . . \n",
            ". . . G \n",
            "\n",
            "\n",
            "--- Step 3 ---\n",
            "Agent choosing action: down\n",
            "Environment processing action: down\n",
            "New state: (0, 1), Reward: -1, Done: False\n",
            "Agent learned: State (0, 1) → Action down → Reward -1\n",
            "Total reward so far: -3\n",
            ". A . . \n",
            ". X . . \n",
            ". X . . \n",
            ". . . G \n",
            "\n",
            "\n",
            "=== RETURNS ===\n",
            "Returns = Total accumulated reward over time\n",
            "Agent's total return: -3\n",
            "Goal: Learn to maximize expected returns\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### value-based method: Q-Learning"
      ],
      "metadata": {
        "id": "8lyfbJNm_y94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "\n",
        "    def __init__(self, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):\n",
        "        self.name = \"Q-Learning Agent\"\n",
        "        self.q_table = defaultdict(lambda: defaultdict(float))  # Q(state, action)\n",
        "        self.learning_rate = learning_rate  # How fast to learn\n",
        "        self.discount_factor = discount_factor  # How much to value future rewards\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.total_reward = 0\n",
        "\n",
        "        print(f\"=== {self.name} ===\")\n",
        "        print(\"Learns by updating Q-values: Q(s,a) = expected return\")\n",
        "        print(f\"Learning rate: {learning_rate}\")\n",
        "        print(f\"Discount factor: {discount_factor}\")\n",
        "        print(f\"Exploration rate: {epsilon}\")\n",
        "        print()\n",
        "\n",
        "    def choose_action(self, state, possible_actions):\n",
        "        if random.random() < self.epsilon:\n",
        "            # Explore: random action\n",
        "            action = random.choice(possible_actions)\n",
        "            print(f\"Exploring: random action {action}\")\n",
        "        else:\n",
        "            # Exploit: best known action\n",
        "            q_values = {action: self.q_table[state][action] for action in possible_actions}\n",
        "            action = max(q_values, key=q_values.get)\n",
        "            print(f\"Exploiting: best action {action} (Q-value: {q_values[action]:.2f})\")\n",
        "\n",
        "        return action\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, possible_actions):\n",
        "\n",
        "        self.total_reward += reward\n",
        "\n",
        "        # Current Q-value\n",
        "        current_q = self.q_table[state][action]\n",
        "\n",
        "        # Best Q-value for next state\n",
        "        if next_state:\n",
        "            max_next_q = max([self.q_table[next_state][a] for a in possible_actions])\n",
        "        else:\n",
        "            max_next_q = 0  # Terminal state\n",
        "\n",
        "        # Q-learning update\n",
        "        target = reward + self.discount_factor * max_next_q\n",
        "        new_q = current_q + self.learning_rate * (target - current_q)\n",
        "\n",
        "        self.q_table[state][action] = new_q\n",
        "\n",
        "        print(f\"Q-Update: Q({state},{action}) = {current_q:.2f} → {new_q:.2f}\")\n",
        "        print(f\"  Target value: {target:.2f}\")\n",
        "\n",
        "    def show_q_table(self):\n",
        "        print(\"=== LEARNED Q-TABLE ===\")\n",
        "        for state in sorted(self.q_table.keys()):\n",
        "            print(f\"State {state}:\")\n",
        "            for action, q_val in self.q_table[state].items():\n",
        "                print(f\"  {action}: {q_val:.2f}\")"
      ],
      "metadata": {
        "id": "dSc2giKK_3I9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Policy-based method: Policy gradient"
      ],
      "metadata": {
        "id": "AR1KeYVm__ZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyGradientAgent:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.name = \"Policy Gradient Agent\"\n",
        "        self.policy_params = defaultdict(lambda: defaultdict(float))  # Policy parameters\n",
        "        self.learning_rate = learning_rate\n",
        "        self.episode_history = []  # Store episode for learning\n",
        "        self.total_reward = 0\n",
        "\n",
        "        print(f\"=== {self.name} ===\")\n",
        "        print(\"Learns policy directly: π(a|s) = probability of action\")\n",
        "        print(f\"Learning rate: {learning_rate}\")\n",
        "        print()\n",
        "\n",
        "    def get_action_probabilities(self, state, possible_actions):\n",
        "        # Get raw scores for each action\n",
        "        scores = [self.policy_params[state][action] for action in possible_actions]\n",
        "\n",
        "        # Softmax to convert to probabilities\n",
        "        exp_scores = [np.exp(score) for score in scores]\n",
        "        total = sum(exp_scores)\n",
        "        probabilities = [exp_score/total for exp_score in exp_scores]\n",
        "\n",
        "        return dict(zip(possible_actions, probabilities))\n",
        "\n",
        "    def choose_action(self, state, possible_actions):\n",
        "        action_probs = self.get_action_probabilities(state, possible_actions)\n",
        "\n",
        "        # Sample action based on probabilities\n",
        "        actions = list(action_probs.keys())\n",
        "        probs = list(action_probs.values())\n",
        "        action = np.random.choice(actions, p=probs)\n",
        "\n",
        "        print(f\"Policy probabilities: {action_probs}\")\n",
        "        print(f\"Sampled action: {action}\")\n",
        "\n",
        "        return action\n",
        "\n",
        "    def store_experience(self, state, action, reward):\n",
        "        \"\"\"Store experience for end-of-episode learning\"\"\"\n",
        "        self.episode_history.append((state, action, reward))\n",
        "        self.total_reward += reward\n",
        "\n",
        "    def learn_from_episode(self, possible_actions):\n",
        "        print(\"=== POLICY GRADIENT UPDATE ===\")\n",
        "\n",
        "        # Calculate returns for each step\n",
        "        returns = []\n",
        "        total_return = 0\n",
        "        for state, action, reward in reversed(self.episode_history):\n",
        "            total_return += reward\n",
        "            returns.append(total_return)\n",
        "        returns.reverse()\n",
        "\n",
        "        # Update policy parameters\n",
        "        for i, (state, action, reward) in enumerate(self.episode_history):\n",
        "            return_val = returns[i]\n",
        "\n",
        "            # Policy gradient update: increase probability of good actions\n",
        "            if return_val > 0:\n",
        "                self.policy_params[state][action] += self.learning_rate * return_val\n",
        "                print(f\"Increasing probability of {action} in state {state}\")\n",
        "            else:\n",
        "                self.policy_params[state][action] -= self.learning_rate * abs(return_val)\n",
        "                print(f\"Decreasing probability of {action} in state {state}\")\n",
        "\n",
        "        # Clear episode history\n",
        "        self.episode_history = []\n",
        "\n",
        "    def show_policy(self, possible_actions):\n",
        "        print(\"=== LEARNED POLICY ===\")\n",
        "        for state in sorted(self.policy_params.keys()):\n",
        "            probs = self.get_action_probabilities(state, possible_actions)\n",
        "            print(f\"State {state}: {probs}\")"
      ],
      "metadata": {
        "id": "aVj5DuSO_-ky"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### compare both method in practise"
      ],
      "metadata": {
        "id": "zeel256TAQOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset environment\n",
        "env = SimpleEnvironment()\n",
        "env.agent_pos = [0, 0]  # Reset agent position\n",
        "\n",
        "print(\"\\n--- Q-Learning Agent Training ---\")\n",
        "q_agent = QLearningAgent()\n",
        "\n",
        "# Train Q-learning agent for a few steps\n",
        "for step in range(5):\n",
        "    state = env.get_state()\n",
        "    action = q_agent.choose_action(state, env.get_possible_actions())\n",
        "    next_state, reward, done = env.step(action)\n",
        "    q_agent.learn(state, action, reward, next_state, env.get_possible_actions())\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "q_agent.show_q_table()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Reset environment for policy gradient agent\n",
        "env.agent_pos = [0, 0]\n",
        "\n",
        "print(\"\\n--- Policy Gradient Agent Training ---\")\n",
        "pg_agent = PolicyGradientAgent()\n",
        "\n",
        "# Train policy gradient agent for one episode\n",
        "episode_steps = 0\n",
        "while episode_steps < 5:\n",
        "    state = env.get_state()\n",
        "    action = pg_agent.choose_action(state, env.get_possible_actions())\n",
        "    next_state, reward, done = env.step(action)\n",
        "    pg_agent.store_experience(state, action, reward)\n",
        "    episode_steps += 1\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "# Learn from the episode\n",
        "pg_agent.learn_from_episode(env.get_possible_actions())\n",
        "pg_agent.show_policy(env.get_possible_actions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NITU5WrAT2P",
        "outputId": "39fa1988-6e2c-459a-80e8-d81a3ce9c5aa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ENVIRONMENT SETUP ===\n",
            "Grid size: 4x4\n",
            "Agent starts at: [0, 0]\n",
            "Goal is at: [3, 3]\n",
            "Obstacles at: [[1, 1], [2, 1]]\n",
            "A . . . \n",
            ". X . . \n",
            ". X . . \n",
            ". . . G \n",
            "\n",
            "\n",
            "\n",
            "--- Q-Learning Agent Training ---\n",
            "=== Q-Learning Agent ===\n",
            "Learns by updating Q-values: Q(s,a) = expected return\n",
            "Learning rate: 0.1\n",
            "Discount factor: 0.9\n",
            "Exploration rate: 0.1\n",
            "\n",
            "Exploiting: best action up (Q-value: 0.00)\n",
            "Environment processing action: up\n",
            "New state: (0, 0), Reward: -1, Done: False\n",
            "Q-Update: Q((0, 0),up) = 0.00 → -0.10\n",
            "  Target value: -1.00\n",
            "Exploiting: best action down (Q-value: 0.00)\n",
            "Environment processing action: down\n",
            "New state: (1, 0), Reward: -1, Done: False\n",
            "Q-Update: Q((0, 0),down) = 0.00 → -0.10\n",
            "  Target value: -1.00\n",
            "Exploiting: best action up (Q-value: 0.00)\n",
            "Environment processing action: up\n",
            "New state: (0, 0), Reward: -1, Done: False\n",
            "Q-Update: Q((1, 0),up) = 0.00 → -0.10\n",
            "  Target value: -1.00\n",
            "Exploiting: best action left (Q-value: 0.00)\n",
            "Environment processing action: left\n",
            "New state: (0, 0), Reward: -1, Done: False\n",
            "Q-Update: Q((0, 0),left) = 0.00 → -0.10\n",
            "  Target value: -1.00\n",
            "Exploiting: best action right (Q-value: 0.00)\n",
            "Environment processing action: right\n",
            "New state: (0, 1), Reward: -1, Done: False\n",
            "Q-Update: Q((0, 0),right) = 0.00 → -0.10\n",
            "  Target value: -1.00\n",
            "=== LEARNED Q-TABLE ===\n",
            "State (0, 0):\n",
            "  up: -0.10\n",
            "  down: -0.10\n",
            "  left: -0.10\n",
            "  right: -0.10\n",
            "State (0, 1):\n",
            "  up: 0.00\n",
            "  down: 0.00\n",
            "  left: 0.00\n",
            "  right: 0.00\n",
            "State (1, 0):\n",
            "  up: -0.10\n",
            "  down: 0.00\n",
            "  left: 0.00\n",
            "  right: 0.00\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Policy Gradient Agent Training ---\n",
            "=== Policy Gradient Agent ===\n",
            "Learns policy directly: π(a|s) = probability of action\n",
            "Learning rate: 0.01\n",
            "\n",
            "Policy probabilities: {'up': np.float64(0.25), 'down': np.float64(0.25), 'left': np.float64(0.25), 'right': np.float64(0.25)}\n",
            "Sampled action: right\n",
            "Environment processing action: right\n",
            "New state: (0, 1), Reward: -1, Done: False\n",
            "Policy probabilities: {'up': np.float64(0.25), 'down': np.float64(0.25), 'left': np.float64(0.25), 'right': np.float64(0.25)}\n",
            "Sampled action: left\n",
            "Environment processing action: left\n",
            "New state: (0, 0), Reward: -1, Done: False\n",
            "Policy probabilities: {'up': np.float64(0.25), 'down': np.float64(0.25), 'left': np.float64(0.25), 'right': np.float64(0.25)}\n",
            "Sampled action: left\n",
            "Environment processing action: left\n",
            "New state: (0, 0), Reward: -1, Done: False\n",
            "Policy probabilities: {'up': np.float64(0.25), 'down': np.float64(0.25), 'left': np.float64(0.25), 'right': np.float64(0.25)}\n",
            "Sampled action: up\n",
            "Environment processing action: up\n",
            "New state: (0, 0), Reward: -1, Done: False\n",
            "Policy probabilities: {'up': np.float64(0.25), 'down': np.float64(0.25), 'left': np.float64(0.25), 'right': np.float64(0.25)}\n",
            "Sampled action: up\n",
            "Environment processing action: up\n",
            "New state: (0, 0), Reward: -1, Done: False\n",
            "=== POLICY GRADIENT UPDATE ===\n",
            "Decreasing probability of right in state (0, 0)\n",
            "Decreasing probability of left in state (0, 1)\n",
            "Decreasing probability of left in state (0, 0)\n",
            "Decreasing probability of up in state (0, 0)\n",
            "Decreasing probability of up in state (0, 0)\n",
            "=== LEARNED POLICY ===\n",
            "State (0, 0): {'up': np.float64(0.24933594313009988), 'down': np.float64(0.25692935307598763), 'left': np.float64(0.24933594313009988), 'right': np.float64(0.2443987606638125)}\n",
            "State (0, 1): {'up': np.float64(0.25247492081124545), 'down': np.float64(0.25247492081124545), 'left': np.float64(0.2425752375662637), 'right': np.float64(0.25247492081124545)}\n"
          ]
        }
      ]
    }
  ]
}