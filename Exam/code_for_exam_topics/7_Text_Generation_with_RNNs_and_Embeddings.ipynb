{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Qoi4uI5XNo1W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Char-RNN to generate text"
      ],
      "metadata": {
        "id": "djl8jve7OwY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"hello\"\n",
        "\n",
        "# Step 1: Preprocess - Create vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Encode text to integer indices\n",
        "encoded_text = [char_to_idx[c] for c in text]\n",
        "\n",
        "# Step 2: Define the CharRNN model\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_size)         # Character Embeddings\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)               # Output layer to vocab size\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embed(x)              # Convert character indices to embeddings\n",
        "        output, hidden = self.rnn(x, hidden)  # Process sequence with RNN\n",
        "        logits = self.fc(output)       # Map to output vocabulary\n",
        "        return logits, hidden\n",
        "\n",
        "# Step 3: Instantiate model\n",
        "hidden_size = 16\n",
        "model = CharRNN(vocab_size, hidden_size)\n",
        "\n",
        "# Step 4: Prepare training data (1-step toy training)\n",
        "input_seq = torch.tensor([encoded_text[:-1]])   # \"hell\"\n",
        "target_seq = torch.tensor([encoded_text[1:]])   # \"ello\"\n",
        "\n",
        "# Step 5: Forward pass\n",
        "logits, _ = model(input_seq)\n",
        "loss = F.cross_entropy(logits.view(-1, vocab_size), target_seq.view(-1))\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "Uwk96VOmOsBI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Role of charcter enbedding and softmax output layer"
      ],
      "metadata": {
        "id": "iMlj3a9CO9jB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Predict next character\n",
        "last_logits = logits[0, -1]              # Output of last time step\n",
        "prob_dist = F.softmax(last_logits, dim=0)\n",
        "\n",
        "next_char_idx = torch.multinomial(prob_dist, num_samples=1).item()\n",
        "next_char = idx_to_char[next_char_idx]\n",
        "\n",
        "print(\"Predicted next char:\", next_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__Ls_LwyPGF-",
        "outputId": "93f2d9b0-95af-48d7-f342-3a4badcaafbc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted next char: e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Temperature sampling influence"
      ],
      "metadata": {
        "id": "HUqk5eR1PI7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Sampling function with temperature\n",
        "def sample_with_temperature(logits, temperature=1.0):\n",
        "    logits = logits / temperature\n",
        "    probs = F.softmax(logits, dim=0)\n",
        "    return torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "# Step 8: Test different temperatures\n",
        "for temp in [0.5, 1.0, 1.5]:\n",
        "    idx = sample_with_temperature(last_logits, temperature=temp)\n",
        "    print(f\"Temp {temp}: {idx_to_char[idx]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mw1rIp98PL2i",
        "outputId": "9c7fe9c6-171a-4372-de1b-549dafe9189d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temp 0.5: e\n",
            "Temp 1.0: l\n",
            "Temp 1.5: o\n"
          ]
        }
      ]
    }
  ]
}