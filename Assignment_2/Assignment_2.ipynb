{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "TGia-psL6uf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppb8QybiV17K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d15c88-344f-4872-d4e7-22a079387c20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "import numpy as np\n",
        "x = np.load(\"/content/drive/My Drive/x_letters.npy\")\n",
        "y=np.load(\"/content/drive/My Drive/y_letters.npy\")"
      ],
      "metadata": {
        "id": "434GqmLYnvOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sISAGusv2Ain",
        "outputId": "1cc4e730-700c-4d20-8761-deb563b753de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((88799, 28, 28), (88799,))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = to_categorical(y, num_classes=26)\n",
        "\n",
        "X_train_full, X_test, Y_train_full, Y_test = train_test_split(x, y, stratify = y\n",
        "                                                              )\n",
        "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train_full, Y_train_full, stratify = Y_train_full)\n",
        "X_train, X_valid, X_test = X_train/255., X_valid/255., X_test/255.\n",
        "\n",
        "print(f\"x_train shape: {X_train.shape}\")\n",
        "print(f\"x_valid shape: {X_valid.shape}\")\n",
        "print(f\"x_test shape: {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCGJdKHc23lU",
        "outputId": "1fb22db7-24ed-4692-e1fe-a9342fb5b338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (49949, 28, 28)\n",
            "x_valid shape: (16650, 28, 28)\n",
            "x_test shape: (22200, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Sample random points in the latent space\n",
        "\n",
        "        real_images, real_classes = data\n",
        "        batch_size = ops.shape(real_images)[0]\n",
        "        random_latent_vectors = keras.random.normal(\n",
        "            shape=(batch_size, self.latent_dim),\n",
        "            seed = self.seed_generator\n",
        "        )\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator([random_latent_vectors, real_classes])\n",
        "\n",
        "        # Combine them with real images\n",
        "        real_images = real_images * 2 - 1  # Rescale to [-1, 1], because the generator outputs images in the same range (using tanh as last activation function)\n",
        "        combined_images = ops.concatenate([generated_images, real_images], axis=0)\n",
        "        combined_classes = ops.concatenate([real_classes, real_classes], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = ops.concatenate(\n",
        "            [ops.ones((batch_size, 1)) * 0.9, ops.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        #basically makes sure the discriminator is not perfect,\n",
        "        #so that the generator never has a chance to learn\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator([combined_images, combined_classes])\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "\n",
        "        # Clip gradients for training stability\n",
        "        # basically makes sure the gradients are not too big\n",
        "        # by clipping them to a certain value if they are over that value\n",
        "        grads = [tf.clip_by_value(g, -1.0, 1.0) for g in grads]\n",
        "\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = keras.random.normal(\n",
        "            shape=(batch_size, self.latent_dim),\n",
        "            seed = self.seed_generator\n",
        "        )\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = ops.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            generated_images = self.generator([random_latent_vectors, real_classes])\n",
        "            predictions = self.discriminator([generated_images, real_classes])\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        # Clip gradients for stability\n",
        "        grads = [tf.clip_by_value(g, -1.0, 1.0) for g in grads]\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }"
      ],
      "metadata": {
        "id": "WLW5PQfm6LsF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}